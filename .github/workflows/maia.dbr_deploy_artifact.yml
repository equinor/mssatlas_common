###############################################################
# This is a reusable workflow. The workflow is callable by 
# by workflows in other repos. This workflow deploys the aritfact
# content uploaded by maia.dbr_build_artifact.yml.
###############################################################

name: Callable - Deploy Databricks

on:
  workflow_call:
    inputs:
      environment:
        description: 'The name of the environment.'
        default: '[MISSING]'
        required: true
        type: string
      key_vault_name:
        description: 'The name of the key vault'
        default: '[MISSING]'
        required: true
        type: string
      squad_name:
        description: 'The name of the squad'
        default: '[MISSING]'
        required: true
        type: string
    secrets:
      AZURE_CLIENT_ID:
        required: true
      AZURE_SUBSCRIPTION_ID:
        required: true
      AZURE_TENANT_ID:
        required: true

jobs:
  deploy:
    runs-on: ubuntu-latest
    environment:
      name: ${{ inputs.environment }}
    steps:
      - name: Download deployment artifact from public repo
        uses: actions/download-artifact@v2
        with:
          name: deploy-template
          path: ./deploy-files

      - name: Download notebook artifact
        uses: actions/download-artifact@v2
        with:
          name: databricks-template
          path: ./artifact

      - name: Display structure of downloaded files
        working-directory: ./artifact
        run: |
          ls -R
          pwd

      - name: 'Az CLI login'
        uses: azure/login@v1
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - uses: Azure/get-keyvault-secrets@v1
        with: 
          keyvault: ${{ inputs.key_vault_name }}
          secrets: 'atlas-maiacmn-databricks-url, atlas-maiacmn-databricks-pat'
        id: SecretAction
      - name: Install Databricks-CLI
        run: |
          pip install --upgrade databricks-cli

      - name: Configure Databricks-CLI
        run: |
          {
          echo '[DEFAULT]'
          echo 'host = ${{steps.SecretAction.outputs.atlas-maiacmn-databricks-url}}'
          echo 'token = ${{steps.SecretAction.outputs.atlas-maiacmn-databricks-pat}}'
          } >> ~/.databrickscfg
          databricks workspace ls
      - name: Upload notebooks
        run: | 
          SQUAD_PATH="/Shared/${{ inputs.squad_name }}/"
          # databricks workspace import_dir -o ./artifact/Notebooks $SQUAD_PATH
      - name: Clean up workspace
        shell: pwsh
        run: |
          ./deploy-files/workspace_cleanup.ps1 -INPUT_FOLDER_NAME ${{ inputs.squad_name }} -SOURCES_PATH ./artifact/Notebooks


      # Deploy Databricks jobs 
      - name: Setup python
        uses: actions/setup-python@v2
        with:
          python-version: 3.9
          cache: 'pip'
      - run: pip install -r ./deploy-files/requirements.txt

      - name: Deploy Databricks jobs
        run: |
         python ./deploy-files/databrick_jobs.py ${{ inputs.squad_name }} ${{ inputs.key_vault_name }} ${{ inputs.environment }}
